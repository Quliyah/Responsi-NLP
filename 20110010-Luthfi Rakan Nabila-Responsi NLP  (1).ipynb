{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42342b1",
   "metadata": {},
   "source": [
    "#Evaluasi CPMK 1 dan  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7477c004",
   "metadata": {},
   "source": [
    "# Soal 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362331b",
   "metadata": {},
   "source": [
    "<br>1.CPMK1- Sub CPMK1.1( bobot : 10 )\n",
    "<br>Lakukan crawling data teks dari media sosial/ web dan simpan hasilnya dalam bentuk excel/csv.\n",
    "<br>Setiap mahasiswa harus melakukan crawling dengan kata kunci tertentu.\n",
    "<br>Kata kunci tidak boleh sama dengan mahasiswa lainnya. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3070b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-play-scraper\n",
      "  Downloading google_play_scraper-1.2.4-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: google-play-scraper\n",
      "Successfully installed google-play-scraper-1.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-play-scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4c0711d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nama Aplikasi: Tokopedia Seller\n",
      "ID Aplikasi: com.tokopedia.sellerapp\n",
      "Link: https://play.google.com/store/apps/details?id=com.tokopedia.sellerapp&hl=en&gl=us\n",
      "---\n",
      "Nama Aplikasi: Mitra Tokopedia: Pulsa & PPOB\n",
      "ID Aplikasi: com.tokopedia.kelontongapp\n",
      "Link: https://play.google.com/store/apps/details?id=com.tokopedia.kelontongapp&hl=en&gl=us\n",
      "---\n",
      "Nama Aplikasi: Tokopedia Academy\n",
      "ID Aplikasi: com.tokopedia.academy\n",
      "Link: https://play.google.com/store/apps/details?id=com.tokopedia.academy&hl=en&gl=us\n",
      "---\n",
      "Nama Aplikasi: Tokopedia START\n",
      "ID Aplikasi: com.tokopedia.eventacademy\n",
      "Link: https://play.google.com/store/apps/details?id=com.tokopedia.eventacademy&hl=en&gl=us\n",
      "---\n",
      "Nama Aplikasi: Sobat Tokopedia\n",
      "ID Aplikasi: com.tokopedia.sobatapp\n",
      "Link: https://play.google.com/store/apps/details?id=com.tokopedia.sobatapp&hl=en&gl=us\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from google_play_scraper import search, app\n",
    "\n",
    "def get_app_data(keyword, num_results=5):\n",
    "    # Cari aplikasi berdasarkan kata kunci\n",
    "    results = search(keyword, lang='id', country='id')\n",
    "    \n",
    "    app_data_list = []\n",
    "    \n",
    "    # Ambil sejumlah elemen yang diinginkan\n",
    "    for result in results[:num_results]:\n",
    "        # Dapatkan informasi aplikasi berdasarkan ID aplikasi\n",
    "        app_data = app(result['appId'])\n",
    "        app_data_list.append(app_data)\n",
    "    \n",
    "    return app_data_list\n",
    "\n",
    "# Contoh penggunaan\n",
    "keyword = 'tokopedia'\n",
    "data_aplikasi = get_app_data(keyword)\n",
    "\n",
    "# Tampilkan hasil\n",
    "for app_data in data_aplikasi:\n",
    "    print(f'Nama Aplikasi: {app_data[\"title\"]}')\n",
    "    print(f'ID Aplikasi: {app_data[\"appId\"]}')\n",
    "    print(f'Link: {app_data[\"url\"]}')\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1ab301",
   "metadata": {},
   "source": [
    "# Soal 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1335850",
   "metadata": {},
   "source": [
    "<br>Lakukan pembersihan meliputi penghapusan punctuation, angka dan karakter yang tidak penting menggunakan menggunakan regex.\n",
    "<br>Simpan hasilnya menjadi file csv/excel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0735d487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nama Aplikasi: Tokopedia Seller\n",
      "ID Aplikasi: comtokopediasellerapp\n",
      "Link: httpsplaygooglecomstoreappsdetailsidcomtokopediasellerapphlenglus\n",
      "---\n",
      "Nama Aplikasi: Mitra Tokopedia Pulsa  PPOB\n",
      "ID Aplikasi: comtokopediakelontongapp\n",
      "Link: httpsplaygooglecomstoreappsdetailsidcomtokopediakelontongapphlenglus\n",
      "---\n",
      "Nama Aplikasi: Tokopedia Academy\n",
      "ID Aplikasi: comtokopediaacademy\n",
      "Link: httpsplaygooglecomstoreappsdetailsidcomtokopediaacademyhlenglus\n",
      "---\n",
      "Nama Aplikasi: Tokopedia START\n",
      "ID Aplikasi: comtokopediaeventacademy\n",
      "Link: httpsplaygooglecomstoreappsdetailsidcomtokopediaeventacademyhlenglus\n",
      "---\n",
      "Nama Aplikasi: Sobat Tokopedia\n",
      "ID Aplikasi: comtokopediasobatapp\n",
      "Link: httpsplaygooglecomstoreappsdetailsidcomtokopediasobatapphlenglus\n",
      "---\n",
      "Hasil pencarian disimpan dalam file: hasil_pencarian.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Nama Aplikasi', 'ID Aplikasi', 'Link'])\n",
    "\n",
    "        for app_data in data:\n",
    "            \n",
    "            cleaned_title = clean_text(app_data['title'])\n",
    "            cleaned_app_id = clean_text(app_data['appId'])\n",
    "            cleaned_url = clean_text(app_data['url'])\n",
    "\n",
    "            csv_writer.writerow([cleaned_title, cleaned_app_id, cleaned_url])\n",
    "\n",
    "# Contoh penggunaan\n",
    "keyword = 'tokopedia'\n",
    "data_aplikasi = get_app_data(keyword)\n",
    "\n",
    "# Tampilkan dan simpan hasil ke CSV\n",
    "output_filename = 'hasil_pencarian.csv'\n",
    "for app_data in data_aplikasi:\n",
    "    print(f'Nama Aplikasi: {clean_text(app_data[\"title\"])}')\n",
    "    print(f'ID Aplikasi: {clean_text(app_data[\"appId\"])}')\n",
    "    print(f'Link: {clean_text(app_data[\"url\"])}')\n",
    "    print('---')\n",
    "\n",
    "# Simpan hasil ke CSV\n",
    "save_to_csv(data_aplikasi, output_filename)\n",
    "print(f'Hasil pencarian disimpan dalam file: {output_filename}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5935b39",
   "metadata": {},
   "source": [
    "# Soal 3a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3511c137",
   "metadata": {},
   "source": [
    "a.\tLakukan parsing dan simpan hasilnya dalam bentuk excel/csv (bobot:10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8e32db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nama Aplikasi: Tokopedia Seller\n",
      "ID Aplikasi: comtokopediasellerapp\n",
      "Link: httpsplaygooglecomstoreappsdetailsidcomtokopediasellerapphlenglus\n",
      "---\n",
      "Nama Aplikasi: Mitra Tokopedia Pulsa  PPOB\n",
      "ID Aplikasi: comtokopediakelontongapp\n",
      "Link: httpsplaygooglecomstoreappsdetailsidcomtokopediakelontongapphlenglus\n",
      "---\n",
      "Nama Aplikasi: Tokopedia Academy\n",
      "ID Aplikasi: comtokopediaacademy\n",
      "Link: httpsplaygooglecomstoreappsdetailsidcomtokopediaacademyhlenglus\n",
      "---\n",
      "Nama Aplikasi: Tokopedia START\n",
      "ID Aplikasi: comtokopediaeventacademy\n",
      "Link: httpsplaygooglecomstoreappsdetailsidcomtokopediaeventacademyhlenglus\n",
      "---\n",
      "Nama Aplikasi: Sobat Tokopedia\n",
      "ID Aplikasi: comtokopediasobatapp\n",
      "Link: httpsplaygooglecomstoreappsdetailsidcomtokopediasobatapphlenglus\n",
      "---\n",
      "Hasil pencarian disimpan dalam file CSV: hasil_pencarian.csv\n",
      "Hasil pencarian disimpan dalam file Excel: hasil_pencarian.xlsx\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def clean_text(text):\n",
    "    # Hapus punctuation, angka, dan karakter yang tidak penting menggunakan regex\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Nama Aplikasi', 'ID Aplikasi', 'Link'])\n",
    "\n",
    "        for app_data in data:\n",
    "            # Lakukan pembersihan teks untuk setiap kolom\n",
    "            cleaned_title = clean_text(app_data['title'])\n",
    "            cleaned_app_id = clean_text(app_data['appId'])\n",
    "            cleaned_url = clean_text(app_data['url'])\n",
    "\n",
    "            csv_writer.writerow([cleaned_title, cleaned_app_id, cleaned_url])\n",
    "\n",
    "def save_to_excel(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "# Contoh penggunaan\n",
    "keyword = 'tokopedia'\n",
    "data_aplikasi = get_app_data(keyword)\n",
    "\n",
    "# Tampilkan dan simpan hasil ke CSV\n",
    "csv_filename = 'hasil_pencarian.csv'\n",
    "excel_filename = 'hasil_pencarian.xlsx'\n",
    "\n",
    "for app_data in data_aplikasi:\n",
    "    print(f'Nama Aplikasi: {clean_text(app_data[\"title\"])}')\n",
    "    print(f'ID Aplikasi: {clean_text(app_data[\"appId\"])}')\n",
    "    print(f'Link: {clean_text(app_data[\"url\"])}')\n",
    "    print('---')\n",
    "\n",
    "# Simpan hasil ke CSV\n",
    "save_to_csv(data_aplikasi, csv_filename)\n",
    "print(f'Hasil pencarian disimpan dalam file CSV: {csv_filename}')\n",
    "\n",
    "# Simpan hasil ke Excel\n",
    "save_to_excel(data_aplikasi, excel_filename)\n",
    "print(f'Hasil pencarian disimpan dalam file Excel: {excel_filename}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520fcef0",
   "metadata": {},
   "source": [
    "# Soal 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d0c1e",
   "metadata": {},
   "source": [
    "b.Carilah kata-kata slangword yang ada dalam dataset Anda, dengan cara mencocokan dengan kamus KBBI (terlampir). \n",
    "<br>Simpan hasilnya dalam bentuk csv/excel.\n",
    "<br> Tampilkan 100 kata slang yang Anda dapatkan dan tampilkan dalam bentuk Gunakan (bobot:25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import csv\n",
    "from math import isnan\n",
    "\n",
    "# Fungsi untuk membersihkan teks dari tanda baca dan karakter khusus\n",
    "def clean_text(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    cleaned_text = text.translate(translator)\n",
    "    return cleaned_text\n",
    "\n",
    "# Fungsi untuk mencari kata-kata slang dalam dataset\n",
    "def find_slang_words(dataset):\n",
    "    slang_words = set()\n",
    "\n",
    "    for column in dataset.columns:\n",
    "        for text in dataset[column]:\n",
    "            # Periksa apakah nilai NaN\n",
    "            if not isnan(text):\n",
    "                # Membagi teks menjadi kata-kata dan membersihkannya\n",
    "                words = clean_text(str(text).lower()).split()\n",
    "\n",
    "                for word in words:\n",
    "                    # Tambahkan kata ke set slang_words jika kata tidak ada di kamus baku\n",
    "                    if len(word) > 1 and word not in set_of_standard_words:\n",
    "                        slang_words.add(word)\n",
    "\n",
    "    return list(slang_words)\n",
    "\n",
    "# Fungsi untuk menyimpan hasil dalam file CSV\n",
    "def save_slang_to_csv(slang_words, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Slang Word'])\n",
    "        \n",
    "        for word in slang_words:\n",
    "            csv_writer.writerow([word])\n",
    "\n",
    "# Fungsi untuk menampilkan 100 kata slang dengan bobot\n",
    "def display_top_slang(slang_words, weight=25):\n",
    "    print(f\"Gunakan (bobot:{weight}):\")\n",
    "    for i, word in enumerate(slang_words[:100]):\n",
    "        print(f\"{i + 1}. {word} (bobot:{weight})\")\n",
    "\n",
    "# Contoh penggunaan\n",
    "dataset_path = 'stopwords.csv'\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# Set kata-kata baku yang dianggap sebagai baseline\n",
    "set_of_standard_words = set([\"kata1\", \"kata2\", \"kata3\"])  # Gantilah dengan kata-kata baku yang sesuai\n",
    "\n",
    "# Temukan kata-kata slang dalam dataset\n",
    "slang_words = find_slang_words(dataset)\n",
    "\n",
    "# Simpan hasil dalam file CSV\n",
    "save_slang_to_csv(slang_words, 'slang_words.csv')\n",
    "\n",
    "# Tampilkan 100 kata slang\n",
    "display_first_100_slang(slang_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562cecb",
   "metadata": {},
   "source": [
    "# Soal 3c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544d5476",
   "metadata": {},
   "source": [
    "c.Lakukan tokenizing berdasarkan  hasil 3b, simpan hasilnya dalam bentuk csv/excel  dan tampilkan 100 token pertama. (bobot:10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "\n",
    "# Fungsi untuk melakukan tokenizing\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Fungsi untuk melakukan tokenizing pada seluruh dataset\n",
    "def tokenize_dataset(dataset):\n",
    "    tokenized_data = []\n",
    "\n",
    "    for column in dataset.columns:\n",
    "        for text in dataset[column]:\n",
    "            # Tokenizing teks\n",
    "            tokens = tokenize_text(str(text).lower())\n",
    "            tokenized_data.extend(tokens)\n",
    "\n",
    "    return tokenized_data\n",
    "\n",
    "# Fungsi untuk menyimpan hasil tokenizing dalam file CSV\n",
    "def save_tokens_to_csv(tokens, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Token'])\n",
    "        \n",
    "        for token in tokens:\n",
    "            csv_writer.writerow([token])\n",
    "\n",
    "# Fungsi untuk menampilkan 100 token pertama\n",
    "def display_first_100_tokens(tokens):\n",
    "    print(\"100 Token Pertama:\")\n",
    "    print(tokens[:100])\n",
    "\n",
    "# Contoh penggunaan\n",
    "# Di sini hasil kata-kata slang dari file CSV, kita ganti 'slang_words.csv' dengan nama file CSV yang sesuai\n",
    "slang_words_path = 'slang_words.csv'\n",
    "slang_words = pd.read_csv(slang_words_path)['Slang Word']\n",
    "\n",
    "# Di sini kita ganti dg file CSV yg udah didownload\n",
    "dataset_path = 'stopwords.csv'\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# Tokenizing berdasarkan hasil 3b\n",
    "tokenized_data = tokenize_dataset(dataset)\n",
    "\n",
    "# Menyimpan hasil tokenizing dalam file CSV\n",
    "save_tokens_to_csv(tokenized_data, 'tokenized_data.csv')\n",
    "\n",
    "# Menampilkan 100 token pertama\n",
    "display_first_100_tokens(tokenized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b05cc",
   "metadata": {},
   "source": [
    "# Soal 3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f7e6f",
   "metadata": {},
   "source": [
    "d.Lakukan stopword removing  berdasarkan hasil 3c, simpan hasilnya dalam bentuk csv/excel. (bobot:10)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208095b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "\n",
    "# Download stopwords jika belum diunduh\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Fungsi untuk melakukan stopword removal\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Fungsi untuk melakukan stopword removal pada seluruh dataset\n",
    "def remove_stopwords_from_dataset(dataset):\n",
    "    filtered_data = []\n",
    "\n",
    "    for column in dataset.columns:\n",
    "        for text in dataset[column]:\n",
    "            # Tokenizing teks\n",
    "            tokens = word_tokenize(str(text).lower())\n",
    "\n",
    "            # Hapus stopwords\n",
    "            filtered_tokens = remove_stopwords(tokens)\n",
    "            filtered_data.extend(filtered_tokens)\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "# Fungsi untuk menyimpan hasil stopword removal dalam file CSV\n",
    "def save_filtered_tokens_to_csv(filtered_tokens, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Filtered Token'])\n",
    "        \n",
    "        for token in filtered_tokens:\n",
    "            csv_writer.writerow([token])\n",
    "\n",
    "# Fungsi untuk menampilkan beberapa token yang telah difilter\n",
    "def display_filtered_tokens(filtered_tokens, num_tokens=10):\n",
    "    print(f\"{num_tokens} Token Setelah Stopword Removal:\")\n",
    "    print(filtered_tokens[:num_tokens])\n",
    "\n",
    "# Contoh penggunaan\n",
    "slang_words_path = 'stopwords.csv'\n",
    "slang_words = pd.read_csv(slang_words_path)['Slang Word']\n",
    "\n",
    "dataset_path = 'stopwords.csv'\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# Tokenizing berdasarkan hasil 3b\n",
    "tokenized_data = tokenize_dataset(dataset)\n",
    "\n",
    "# Menghapus kata-kata slang dari tokenized_data\n",
    "filtered_data = [token for token in tokenized_data if token not in slang_words]\n",
    "\n",
    "# Melakukan stopword removal\n",
    "filtered_tokens = remove_stopwords(filtered_data)\n",
    "\n",
    "# Menyimpan hasil stopword removal dalam file CSV\n",
    "save_filtered_tokens_to_csv(filtered_tokens, 'filtered_tokens.csv')\n",
    "\n",
    "# Menampilkan beberapa token yang telah difilter\n",
    "display_filtered_tokens(filtered_tokens, num_tokens=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc76bb46",
   "metadata": {},
   "source": [
    "# Soal 3e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03bd8b5",
   "metadata": {},
   "source": [
    "e.Lakukan stemming berdasarkan hasil 3d dan tampilkan 100 stem pertama. (bobot:10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf252696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import csv\n",
    "\n",
    "# Fungsi untuk melakukan stemming\n",
    "def stem_tokens(tokens):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "# Fungsi untuk melakukan stemming pada seluruh dataset\n",
    "def stem_dataset(dataset):\n",
    "    stemmed_data = []\n",
    "\n",
    "    for column in dataset.columns:\n",
    "        for text in dataset[column]:\n",
    "            # Tokenizing teks\n",
    "            tokens = word_tokenize(str(text).lower())\n",
    "\n",
    "            # Melakukan stemming\n",
    "            stemmed_tokens = stem_tokens(tokens)\n",
    "            stemmed_data.extend(stemmed_tokens)\n",
    "\n",
    "    return stemmed_data\n",
    "\n",
    "# Fungsi untuk menyimpan hasil stemming dalam file CSV\n",
    "def save_stemmed_tokens_to_csv(stemmed_tokens, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Stemmed Token'])\n",
    "        \n",
    "        for token in stemmed_tokens:\n",
    "            csv_writer.writerow([token])\n",
    "\n",
    "# Fungsi untuk menampilkan 100 stem pertama\n",
    "def display_first_100_stems(stemmed_tokens):\n",
    "    print(\"100 Stem Pertama:\")\n",
    "    print(stemmed_tokens[:100])\n",
    "\n",
    "# Contoh penggunaan\n",
    "slang_words_path = 'stopwords.csv'\n",
    "slang_words = pd.read_csv(stopwords_path)['stopwords']\n",
    "\n",
    "dataset_path = 'stopwords.csv'\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# Tokenizing berdasarkan hasil 3b\n",
    "tokenized_data = tokenize_dataset(dataset)\n",
    "\n",
    "# Menghapus kata-kata slang dari tokenized_data\n",
    "filtered_data = [token for token in tokenized_data if token not in slang_words]\n",
    "\n",
    "# Melakukan stopword removal\n",
    "filtered_tokens = remove_stopwords(filtered_data)\n",
    "\n",
    "# Melakukan stemming\n",
    "stemmed_tokens = stem_tokens(filtered_tokens)\n",
    "\n",
    "# Menyimpan hasil stemming dalam file CSV\n",
    "save_stemmed_tokens_to_csv(stemmed_tokens, 'stemmed_tokens.csv')\n",
    "\n",
    "# Menampilkan 100 stem pertama\n",
    "display_first_100_stems(stemmed_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
